{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bd64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import zipfile \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Model \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb82b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_zip = \"/springboard/Unit 24/mask.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall(\"/springboard/Unit 24/\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2bf9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dic = \"E:/springboard/Unit 24/mask\"\n",
    "train_dic = os.path.join(base_dic, \"train\")\n",
    "validation_dic = os.path.join(base_dic, \"validation\")\n",
    "\n",
    "train_mask_dic = os.path.join(train_dic, \"with_mask\")\n",
    "\n",
    "train_non_mask_dic = os.path.join(train_dic, \"without_mask\")\n",
    "\n",
    "validation_mask_dic = os.path.join(validation_dic, \"with_mask\")\n",
    "\n",
    "validation_non_mask_dic = os.path.join(validation_dic, \"without_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f11afec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "import matplotlib.image as mpimg\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "pic_index = 100\n",
    "train_mask_fnames = os.listdir( train_mask_dic )\n",
    "train_non_mask_fnames = os.listdir( train_non_mask_dic )\n",
    "\n",
    "\n",
    "next_mask_pix = [os.path.join(train_mask_dic, fname) \n",
    "                for fname in train_mask_dic[ pic_index-8:pic_index] \n",
    "               ]\n",
    "\n",
    "next_non_mask_pix = [os.path.join(train_non_mask_fnames, fname) \n",
    "                for fname in train_non_mask_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "\n",
    "for i, img_path in enumerate(next_mask_pix + next_non_mask_pix):\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f24e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale all picture size\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                                   shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6267d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 2 classes.\n",
      "Found 318 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dic, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dic,  batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192f8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "base_model = VGG16(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50950b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ed9aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lv164\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba580276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  7/100 [=>............................] - ETA: 1:46 - loss: 1.8096 - acc: 0.5656WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 30s 278ms/step - loss: 1.8096 - acc: 0.5656 - val_loss: 0.5555 - val_acc: 0.6572\n"
     ]
    }
   ],
   "source": [
    "vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3319a5",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad02ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2,shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5489bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 2 classes.\n",
      "Found 318 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dic, batch_size = 20, class_mode = 'binary', target_size = (150, 150))\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dic, batch_size = 20, class_mode = 'binary', target_size = (150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df726295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "710349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acd82080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e37ccc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-8d91753fef64>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  inc_history = model.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  7/100 [=>............................] - ETA: 38s - loss: 6.4209 - acc: 0.6885WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 11s 77ms/step - loss: 6.4209 - acc: 0.6885 - val_loss: 2.1950 - val_acc: 0.7264\n"
     ]
    }
   ],
   "source": [
    "inc_history = model.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd58a8",
   "metadata": {},
   "source": [
    "# ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "001e6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 2 classes.\n",
      "Found 318 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dic, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dic, batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c74b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8223afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab7bdc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD \n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "base_model = Sequential()\n",
    "base_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
    "base_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9686be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lv164\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "base_model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3787a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  7/100 [=>............................] - ETA: 3:48 - loss: 2.6425 - acc: 0.5656WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 33s 271ms/step - loss: 2.6425 - acc: 0.5656 - val_loss: 1.0777 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "resnet_history = base_model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ca41f",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10f0c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54e3da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 2 classes.\n",
      "Found 318 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dic, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dic, batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d9638af",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e7d2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d601d1c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-d32eb69c696c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1043\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1044\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    664\u001b[0m       \u001b[1;31m# Full static shape is guaranteed to be available.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m       \u001b[1;31m# Performance: Using `constant_op` is much faster than passing a list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m       \u001b[0mflattened_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflattened_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mConstant\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m   \u001b[0mRaises\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mincorrectly\u001b[0m \u001b[0mspecified\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0munsupported\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[1;34m\"\"\"Implementation of constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m       \u001b[0mconst_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "model_final = Model(input = base_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbc041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
